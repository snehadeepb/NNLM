{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-MN96RCkTkS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ac_lB75rq6b"
      },
      "outputs": [],
      "source": [
        "# df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1UYTWLJkYiM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90cb2a65-95ac-484b-a3d8-1bc1a1a410cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-81-d99250409226>:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  y = df['class'].replace({'spam':0,'ham':1})\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('spam.csv')\n",
        "df =df.iloc[:1200, :]\n",
        "X = df['text']\n",
        "y = df['class'].replace({'spam':0,'ham':1})\n",
        "\n",
        "sentences = X.tolist()\n",
        "target = y.tolist()\n",
        "\n",
        "word_list = \" \".join(sentences).split()\n",
        "word_list = list(set(word_list))+['spam','ham']\n",
        "word_dict = {w: i for i, w in enumerate(word_list)}\n",
        "number_dict = {i: w for i, w in enumerate(word_list)}\n",
        "n_class = len(word_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNzNfoVfxuVm",
        "outputId": "a0f150e9-ec74-4a25-b8a9-db8876ca0b6c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "910"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "pdd = max(list(map(len,sentences)))\n",
        "pdd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBRGcTwayt3g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-xGmshWsQ3X"
      },
      "outputs": [],
      "source": [
        "# x = [torch.nn.functional.pad(torch.tensor([word_dict[j] for j in i.split()],dtype=torch.float64),(pdd-len([word_dict[j] for j in i.split()]),0)) for i in sentences ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MblW-tU00siu",
        "outputId": "88d5b31d-1c26-4ba2-fde6-b46792c81cd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1789, 2944,  672,  ...,    0,    0,    0],\n",
            "        [ 128, 4183, 3611,  ...,    0,    0,    0],\n",
            "        [3969, 5117, 4263,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [1964,   52, 1254,  ...,    0,    0,    0],\n",
            "        [ 640, 1194, 1132,  ...,    0,    0,    0],\n",
            "        [5488, 4607,  883,  ...,    0,    0,    0]])\n"
          ]
        }
      ],
      "source": [
        "#####################################gemini\n",
        "import torch\n",
        "\n",
        "# Assuming x, pdd, word_dict, and sentences are defined as in your original code\n",
        "\n",
        "# Modify the padding to pad on the right side only\n",
        "x = [torch.nn.functional.pad(torch.tensor([word_dict[j] for j in i.split()], dtype=torch.int64), (0, pdd - len([word_dict[j] for j in i.split()]))) for i in sentences]\n",
        "\n",
        "\n",
        "x_tensor = torch.stack(x) # Use torch.stack to combine tensors of the same shape into a single tensor\n",
        "\n",
        "# Now x_tensor should be a valid PyTorch tensor\n",
        "print(x_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE5HYhBUuS73",
        "outputId": "9b460282-3ffa-4cbb-ddb9-fcd0eb707406"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1200, 910]), torch.Size([1200]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "\n",
        "input = torch.stack(x)\n",
        "output = torch.from_numpy(np.array(y))\n",
        "input.shape,output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgP7F2Kzrr_3",
        "outputId": "7c3d9f4a-e8ca-4505-a86b-d4207493d76e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1200, 910]), torch.Size([1200]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "input.shape , output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8tn6iLyi0mD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32dd4c87-263a-400d-d57f-4140f934e6ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[1789, 2944,  672,  ...,    0,    0,    0],\n",
              "         [ 128, 4183, 3611,  ...,    0,    0,    0],\n",
              "         [3969, 5117, 4263,  ...,    0,    0,    0]]),\n",
              " tensor([1, 1, 0]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# 1. INPUT  OR OUTPUT TENSOR CONNECTION EACH OTHER\n",
        "from torch.utils.data import TensorDataset\n",
        "train_ds = TensorDataset(input , output)\n",
        "train_ds[0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOHCPUnJj8SZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22b03107-859b-4d9f-ae4c-461d1b17b2f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3850, 1194, 3684,  ...,    0,    0,    0],\n",
            "        [3850, 2719, 4992,  ...,    0,    0,    0],\n",
            "        [4046, 5045,  844,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [2088,  445, 3490,  ...,    0,    0,    0],\n",
            "        [  22,  599, 3528,  ...,    0,    0,    0],\n",
            "        [2841,  522, 1947,  ...,    0,    0,    0]])\n",
            "tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
            "        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n"
          ]
        }
      ],
      "source": [
        "# 2. BATCH DATA USING DATA LOADER\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "bs = 100\n",
        "train_loader = DataLoader(train_ds, batch_size = bs, shuffle = True)\n",
        "for xy, yb in train_loader:\n",
        "    print(xy)\n",
        "    print(yb)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # 3. MODEL CREATION\n",
        "n_step = 2 # number of steps, n-1 in paper\n",
        "n_hidden = 2 # number of hidden size, h in paper\n",
        "m = 910 # embedding size, m in paper\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model,self).__init__()\n",
        "    self.C = nn.Embedding(n_class, m) # n_class as the vocabulary size  and m as the embedding dimension.\n",
        "    self.r = torch.nn.RNN(m,32,batch_first=True) # hidden size of 32. # batch dimension as the first dimension\n",
        "    self.H = nn.Linear( 32, 1, bias=False) #linear layer with an input size of 32 output size of 1.bias=False means this layer does not include a bias term\n",
        "  def forward(self,X):\n",
        "        X = self.C(X) # X : [batch_size, n_step, m]\n",
        "        X,_ = self.r(X) # X : [batch_size, n_step, m]\n",
        "        # print(X[:,-1,:])\n",
        "        X = X[:,-1,:]\n",
        "        a = self.H(X) # X : [batch_size, n_step, m]\n",
        "        return torch.nn.functional.sigmoid(a)\n",
        "\n",
        "model = Model()\n",
        "model(xy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mdSprMucjZK",
        "outputId": "ff3e8c7b-47a5-4f47-ee65-164e0ac8fa3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509],\n",
              "        [0.6509]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saOPKPMKj-y0"
      },
      "outputs": [],
      "source": [
        "# 4. LOSS CALCULATION\n",
        "\n",
        "criterion = nn.BCELoss() # Binary Cross Entropy Loss\n",
        "\"\"\"\n",
        "Binary Cross Entropy Loss (BCE Loss) is a loss function commonly used in binary classification problems where the goal\n",
        " is to predict one of two possible classes (e.g., 0 or 1). This loss function measures the difference between\n",
        " the predicted probabilities and the actual binary labels.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "M9weKqNmvYYc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fx05XS57vYqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwHj2JaCkAW3"
      },
      "outputs": [],
      "source": [
        "# 5. OPTIMIZER\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\"\"\"\n",
        "Adaptive Learning Rates: Adam computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. This means that parameters that have been updated less frequently receive larger updates, while parameters that have been updated frequently receive smaller updates.\n",
        "Bias Correction: Adam includes bias-correction terms to compensate for the initialization of the first and second moment estimates. This helps stabilize the learning process, especially in the early stages of training.\n",
        "Benefits:\n",
        "\n",
        "Adam generally converges faster than traditional optimizers like Stochastic Gradient Descent (SGD), especially for problems with large datasets and/or high-dimensional parameter spaces.\n",
        "It is less sensitive to the choice of hyperparameters compared to other optimizers.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4LZ66VhkBMi"
      },
      "outputs": [],
      "source": [
        "# # 6. EPOCHES\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2aUHlnzhzvGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "o8EOYMoazuv4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9k4MkXqe439",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71cb5c94-4893-44bf-b10d-fe40f8c81685"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.4227\n",
            "Epoch [1/50], Loss: 0.4053\n",
            "Epoch [1/50], Loss: 0.4227\n",
            "Epoch [1/50], Loss: 0.4926\n",
            "Epoch [1/50], Loss: 0.4751\n",
            "Epoch [1/50], Loss: 0.2835\n",
            "Epoch [1/50], Loss: 0.4750\n",
            "Epoch [1/50], Loss: 0.4924\n",
            "Epoch [1/50], Loss: 0.4401\n",
            "Epoch [1/50], Loss: 0.3360\n",
            "Epoch [1/50], Loss: 0.4574\n",
            "Epoch [1/50], Loss: 0.3880\n",
            "Epoch [2/50], Loss: 0.5094\n",
            "Epoch [2/50], Loss: 0.3362\n",
            "Epoch [2/50], Loss: 0.4227\n",
            "Epoch [2/50], Loss: 0.3534\n",
            "Epoch [2/50], Loss: 0.4922\n",
            "Epoch [2/50], Loss: 0.4053\n",
            "Epoch [2/50], Loss: 0.4401\n",
            "Epoch [2/50], Loss: 0.4749\n",
            "Epoch [2/50], Loss: 0.3706\n",
            "Epoch [2/50], Loss: 0.4227\n",
            "Epoch [2/50], Loss: 0.5095\n",
            "Epoch [2/50], Loss: 0.3534\n",
            "Epoch [3/50], Loss: 0.4747\n",
            "Epoch [3/50], Loss: 0.3363\n",
            "Epoch [3/50], Loss: 0.4573\n",
            "Epoch [3/50], Loss: 0.3881\n",
            "Epoch [3/50], Loss: 0.4920\n",
            "Epoch [3/50], Loss: 0.3535\n",
            "Epoch [3/50], Loss: 0.4227\n",
            "Epoch [3/50], Loss: 0.4401\n",
            "Epoch [3/50], Loss: 0.3880\n",
            "Epoch [3/50], Loss: 0.3532\n",
            "Epoch [3/50], Loss: 0.6145\n",
            "Epoch [3/50], Loss: 0.3706\n",
            "Epoch [4/50], Loss: 0.4401\n",
            "Epoch [4/50], Loss: 0.4054\n",
            "Epoch [4/50], Loss: 0.4054\n",
            "Epoch [4/50], Loss: 0.5094\n",
            "Epoch [4/50], Loss: 0.4227\n",
            "Epoch [4/50], Loss: 0.5607\n",
            "Epoch [4/50], Loss: 0.4227\n",
            "Epoch [4/50], Loss: 0.3375\n",
            "Epoch [4/50], Loss: 0.4398\n",
            "Epoch [4/50], Loss: 0.3209\n",
            "Epoch [4/50], Loss: 0.4908\n",
            "Epoch [4/50], Loss: 0.3378\n",
            "Epoch [5/50], Loss: 0.4228\n",
            "Epoch [5/50], Loss: 0.4740\n",
            "Epoch [5/50], Loss: 0.4398\n",
            "Epoch [5/50], Loss: 0.3715\n",
            "Epoch [5/50], Loss: 0.4741\n",
            "Epoch [5/50], Loss: 0.4056\n",
            "Epoch [5/50], Loss: 0.3542\n",
            "Epoch [5/50], Loss: 0.4227\n",
            "Epoch [5/50], Loss: 0.5089\n",
            "Epoch [5/50], Loss: 0.4400\n",
            "Epoch [5/50], Loss: 0.4227\n",
            "Epoch [5/50], Loss: 0.3538\n",
            "Epoch [6/50], Loss: 0.3537\n",
            "Epoch [6/50], Loss: 0.3707\n",
            "Epoch [6/50], Loss: 0.4227\n",
            "Epoch [6/50], Loss: 0.3878\n",
            "Epoch [6/50], Loss: 0.4227\n",
            "Epoch [6/50], Loss: 0.4051\n",
            "Epoch [6/50], Loss: 0.4228\n",
            "Epoch [6/50], Loss: 0.4939\n",
            "Epoch [6/50], Loss: 0.4584\n",
            "Epoch [6/50], Loss: 0.4406\n",
            "Epoch [6/50], Loss: 0.3873\n",
            "Epoch [6/50], Loss: 0.5293\n",
            "Epoch [7/50], Loss: 0.4051\n",
            "Epoch [7/50], Loss: 0.3699\n",
            "Epoch [7/50], Loss: 0.4756\n",
            "Epoch [7/50], Loss: 0.3876\n",
            "Epoch [7/50], Loss: 0.4403\n",
            "Epoch [7/50], Loss: 0.4052\n",
            "Epoch [7/50], Loss: 0.4053\n",
            "Epoch [7/50], Loss: 0.3704\n",
            "Epoch [7/50], Loss: 0.5100\n",
            "Epoch [7/50], Loss: 0.3879\n",
            "Epoch [7/50], Loss: 0.5097\n",
            "Epoch [7/50], Loss: 0.4227\n",
            "Epoch [8/50], Loss: 0.3708\n",
            "Epoch [8/50], Loss: 0.4227\n",
            "Epoch [8/50], Loss: 0.4745\n",
            "Epoch [8/50], Loss: 0.4744\n",
            "Epoch [8/50], Loss: 0.4399\n",
            "Epoch [8/50], Loss: 0.3715\n",
            "Epoch [8/50], Loss: 0.4569\n",
            "Epoch [8/50], Loss: 0.4398\n",
            "Epoch [8/50], Loss: 0.3549\n",
            "Epoch [8/50], Loss: 0.5077\n",
            "Epoch [8/50], Loss: 0.3381\n",
            "Epoch [8/50], Loss: 0.4398\n",
            "Epoch [9/50], Loss: 0.3889\n",
            "Epoch [9/50], Loss: 0.4058\n",
            "Epoch [9/50], Loss: 0.3546\n",
            "Epoch [9/50], Loss: 0.4912\n",
            "Epoch [9/50], Loss: 0.4056\n",
            "Epoch [9/50], Loss: 0.4915\n",
            "Epoch [9/50], Loss: 0.4055\n",
            "Epoch [9/50], Loss: 0.3194\n",
            "Epoch [9/50], Loss: 0.4400\n",
            "Epoch [9/50], Loss: 0.4400\n",
            "Epoch [9/50], Loss: 0.4922\n",
            "Epoch [9/50], Loss: 0.4575\n",
            "Epoch [10/50], Loss: 0.4574\n",
            "Epoch [10/50], Loss: 0.4227\n",
            "Epoch [10/50], Loss: 0.3536\n",
            "Epoch [10/50], Loss: 0.4227\n",
            "Epoch [10/50], Loss: 0.3881\n",
            "Epoch [10/50], Loss: 0.4054\n",
            "Epoch [10/50], Loss: 0.4574\n",
            "Epoch [10/50], Loss: 0.3532\n",
            "Epoch [10/50], Loss: 0.4750\n",
            "Epoch [10/50], Loss: 0.5099\n",
            "Epoch [10/50], Loss: 0.4401\n",
            "Epoch [10/50], Loss: 0.4054\n",
            "Epoch [11/50], Loss: 0.3707\n",
            "Epoch [11/50], Loss: 0.4400\n",
            "Epoch [11/50], Loss: 0.4400\n",
            "Epoch [11/50], Loss: 0.4920\n",
            "Epoch [11/50], Loss: 0.3882\n",
            "Epoch [11/50], Loss: 0.3882\n",
            "Epoch [11/50], Loss: 0.4572\n",
            "Epoch [11/50], Loss: 0.3883\n",
            "Epoch [11/50], Loss: 0.4399\n",
            "Epoch [11/50], Loss: 0.3883\n",
            "Epoch [11/50], Loss: 0.3883\n",
            "Epoch [11/50], Loss: 0.5090\n",
            "Epoch [12/50], Loss: 0.4744\n",
            "Epoch [12/50], Loss: 0.4227\n",
            "Epoch [12/50], Loss: 0.3712\n",
            "Epoch [12/50], Loss: 0.3197\n",
            "Epoch [12/50], Loss: 0.5088\n",
            "Epoch [12/50], Loss: 0.3710\n",
            "Epoch [12/50], Loss: 0.5435\n",
            "Epoch [12/50], Loss: 0.4916\n",
            "Epoch [12/50], Loss: 0.4571\n",
            "Epoch [12/50], Loss: 0.3886\n",
            "Epoch [12/50], Loss: 0.3375\n",
            "Epoch [12/50], Loss: 0.4057\n",
            "Epoch [13/50], Loss: 0.5252\n",
            "Epoch [13/50], Loss: 0.4057\n",
            "Epoch [13/50], Loss: 0.4228\n",
            "Epoch [13/50], Loss: 0.4058\n",
            "Epoch [13/50], Loss: 0.4908\n",
            "Epoch [13/50], Loss: 0.4228\n",
            "Epoch [13/50], Loss: 0.4058\n",
            "Epoch [13/50], Loss: 0.3211\n",
            "Epoch [13/50], Loss: 0.4568\n",
            "Epoch [13/50], Loss: 0.3887\n",
            "Epoch [13/50], Loss: 0.4569\n",
            "Epoch [13/50], Loss: 0.3885\n",
            "Epoch [14/50], Loss: 0.2853\n",
            "Epoch [14/50], Loss: 0.4054\n",
            "Epoch [14/50], Loss: 0.3879\n",
            "Epoch [14/50], Loss: 0.4753\n",
            "Epoch [14/50], Loss: 0.5813\n",
            "Epoch [14/50], Loss: 0.3699\n",
            "Epoch [14/50], Loss: 0.3523\n",
            "Epoch [14/50], Loss: 0.5111\n",
            "Epoch [14/50], Loss: 0.4581\n",
            "Epoch [14/50], Loss: 0.4580\n",
            "Epoch [14/50], Loss: 0.4755\n",
            "Epoch [14/50], Loss: 0.3352\n",
            "Epoch [15/50], Loss: 0.4052\n",
            "Epoch [15/50], Loss: 0.3878\n",
            "Epoch [15/50], Loss: 0.4402\n",
            "Epoch [15/50], Loss: 0.4052\n",
            "Epoch [15/50], Loss: 0.4577\n",
            "Epoch [15/50], Loss: 0.4053\n",
            "Epoch [15/50], Loss: 0.4402\n",
            "Epoch [15/50], Loss: 0.4401\n",
            "Epoch [15/50], Loss: 0.4053\n",
            "Epoch [15/50], Loss: 0.4575\n",
            "Epoch [15/50], Loss: 0.4921\n",
            "Epoch [15/50], Loss: 0.3535\n",
            "Epoch [16/50], Loss: 0.3882\n",
            "Epoch [16/50], Loss: 0.4573\n",
            "Epoch [16/50], Loss: 0.4400\n",
            "Epoch [16/50], Loss: 0.5433\n",
            "Epoch [16/50], Loss: 0.4742\n",
            "Epoch [16/50], Loss: 0.3887\n",
            "Epoch [16/50], Loss: 0.3549\n",
            "Epoch [16/50], Loss: 0.4228\n",
            "Epoch [16/50], Loss: 0.4398\n",
            "Epoch [16/50], Loss: 0.5244\n",
            "Epoch [16/50], Loss: 0.3217\n",
            "Epoch [16/50], Loss: 0.3386\n",
            "Epoch [17/50], Loss: 0.4228\n",
            "Epoch [17/50], Loss: 0.4737\n",
            "Epoch [17/50], Loss: 0.4228\n",
            "Epoch [17/50], Loss: 0.4058\n",
            "Epoch [17/50], Loss: 0.4228\n",
            "Epoch [17/50], Loss: 0.4398\n",
            "Epoch [17/50], Loss: 0.4227\n",
            "Epoch [17/50], Loss: 0.4399\n",
            "Epoch [17/50], Loss: 0.4571\n",
            "Epoch [17/50], Loss: 0.3541\n",
            "Epoch [17/50], Loss: 0.4399\n",
            "Epoch [17/50], Loss: 0.3883\n",
            "Epoch [18/50], Loss: 0.4918\n",
            "Epoch [18/50], Loss: 0.5436\n",
            "Epoch [18/50], Loss: 0.3883\n",
            "Epoch [18/50], Loss: 0.4914\n",
            "Epoch [18/50], Loss: 0.4227\n",
            "Epoch [18/50], Loss: 0.3545\n",
            "Epoch [18/50], Loss: 0.3205\n",
            "Epoch [18/50], Loss: 0.4057\n",
            "Epoch [18/50], Loss: 0.4227\n",
            "Epoch [18/50], Loss: 0.4399\n",
            "Epoch [18/50], Loss: 0.4055\n",
            "Epoch [18/50], Loss: 0.4054\n",
            "Epoch [19/50], Loss: 0.3533\n",
            "Epoch [19/50], Loss: 0.3879\n",
            "Epoch [19/50], Loss: 0.4403\n",
            "Epoch [19/50], Loss: 0.3876\n",
            "Epoch [19/50], Loss: 0.5112\n",
            "Epoch [19/50], Loss: 0.4936\n",
            "Epoch [19/50], Loss: 0.4581\n",
            "Epoch [19/50], Loss: 0.5639\n",
            "Epoch [19/50], Loss: 0.3702\n",
            "Epoch [19/50], Loss: 0.4402\n",
            "Epoch [19/50], Loss: 0.3011\n",
            "Epoch [19/50], Loss: 0.3880\n",
            "Epoch [20/50], Loss: 0.4053\n",
            "Epoch [20/50], Loss: 0.4575\n",
            "Epoch [20/50], Loss: 0.3879\n",
            "Epoch [20/50], Loss: 0.4923\n",
            "Epoch [20/50], Loss: 0.4053\n",
            "Epoch [20/50], Loss: 0.4053\n",
            "Epoch [20/50], Loss: 0.3880\n",
            "Epoch [20/50], Loss: 0.4227\n",
            "Epoch [20/50], Loss: 0.4053\n",
            "Epoch [20/50], Loss: 0.3879\n",
            "Epoch [20/50], Loss: 0.4751\n",
            "Epoch [20/50], Loss: 0.4576\n",
            "Epoch [21/50], Loss: 0.4227\n",
            "Epoch [21/50], Loss: 0.4576\n",
            "Epoch [21/50], Loss: 0.5271\n",
            "Epoch [21/50], Loss: 0.3535\n",
            "Epoch [21/50], Loss: 0.4400\n",
            "Epoch [21/50], Loss: 0.4227\n",
            "Epoch [21/50], Loss: 0.4743\n",
            "Epoch [21/50], Loss: 0.3714\n",
            "Epoch [21/50], Loss: 0.4056\n",
            "Epoch [21/50], Loss: 0.4057\n",
            "Epoch [21/50], Loss: 0.3886\n",
            "Epoch [21/50], Loss: 0.4227\n",
            "Epoch [22/50], Loss: 0.3541\n",
            "Epoch [22/50], Loss: 0.4916\n",
            "Epoch [22/50], Loss: 0.3538\n",
            "Epoch [22/50], Loss: 0.4054\n",
            "Epoch [22/50], Loss: 0.3533\n",
            "Epoch [22/50], Loss: 0.4402\n",
            "Epoch [22/50], Loss: 0.4753\n",
            "Epoch [22/50], Loss: 0.3350\n",
            "Epoch [22/50], Loss: 0.4933\n",
            "Epoch [22/50], Loss: 0.4581\n",
            "Epoch [22/50], Loss: 0.4581\n",
            "Epoch [22/50], Loss: 0.4757\n",
            "Epoch [23/50], Loss: 0.3876\n",
            "Epoch [23/50], Loss: 0.3525\n",
            "Epoch [23/50], Loss: 0.4227\n",
            "Epoch [23/50], Loss: 0.4579\n",
            "Epoch [23/50], Loss: 0.4227\n",
            "Epoch [23/50], Loss: 0.4754\n",
            "Epoch [23/50], Loss: 0.3877\n",
            "Epoch [23/50], Loss: 0.3878\n",
            "Epoch [23/50], Loss: 0.4576\n",
            "Epoch [23/50], Loss: 0.4053\n",
            "Epoch [23/50], Loss: 0.4227\n",
            "Epoch [23/50], Loss: 0.5098\n",
            "Epoch [24/50], Loss: 0.4227\n",
            "Epoch [24/50], Loss: 0.3881\n",
            "Epoch [24/50], Loss: 0.5091\n",
            "Epoch [24/50], Loss: 0.3883\n",
            "Epoch [24/50], Loss: 0.3713\n",
            "Epoch [24/50], Loss: 0.3713\n",
            "Epoch [24/50], Loss: 0.4056\n",
            "Epoch [24/50], Loss: 0.3539\n",
            "Epoch [24/50], Loss: 0.5436\n",
            "Epoch [24/50], Loss: 0.4918\n",
            "Epoch [24/50], Loss: 0.4399\n",
            "Epoch [24/50], Loss: 0.4055\n",
            "Epoch [25/50], Loss: 0.3541\n",
            "Epoch [25/50], Loss: 0.4399\n",
            "Epoch [25/50], Loss: 0.4742\n",
            "Epoch [25/50], Loss: 0.3541\n",
            "Epoch [25/50], Loss: 0.4742\n",
            "Epoch [25/50], Loss: 0.3541\n",
            "Epoch [25/50], Loss: 0.3883\n",
            "Epoch [25/50], Loss: 0.4227\n",
            "Epoch [25/50], Loss: 0.4573\n",
            "Epoch [25/50], Loss: 0.3708\n",
            "Epoch [25/50], Loss: 0.5269\n",
            "Epoch [25/50], Loss: 0.4748\n",
            "Epoch [26/50], Loss: 0.3188\n",
            "Epoch [26/50], Loss: 0.4401\n",
            "Epoch [26/50], Loss: 0.4574\n",
            "Epoch [26/50], Loss: 0.4401\n",
            "Epoch [26/50], Loss: 0.4400\n",
            "Epoch [26/50], Loss: 0.3189\n",
            "Epoch [26/50], Loss: 0.4574\n",
            "Epoch [26/50], Loss: 0.6482\n",
            "Epoch [26/50], Loss: 0.4572\n",
            "Epoch [26/50], Loss: 0.3543\n",
            "Epoch [26/50], Loss: 0.3887\n",
            "Epoch [26/50], Loss: 0.3717\n",
            "Epoch [27/50], Loss: 0.3547\n",
            "Epoch [27/50], Loss: 0.3716\n",
            "Epoch [27/50], Loss: 0.3714\n",
            "Epoch [27/50], Loss: 0.4055\n",
            "Epoch [27/50], Loss: 0.5612\n",
            "Epoch [27/50], Loss: 0.5093\n",
            "Epoch [27/50], Loss: 0.3881\n",
            "Epoch [27/50], Loss: 0.5609\n",
            "Epoch [27/50], Loss: 0.4227\n",
            "Epoch [27/50], Loss: 0.4056\n",
            "Epoch [27/50], Loss: 0.3887\n",
            "Epoch [27/50], Loss: 0.3547\n",
            "Epoch [28/50], Loss: 0.3035\n",
            "Epoch [28/50], Loss: 0.4056\n",
            "Epoch [28/50], Loss: 0.4399\n",
            "Epoch [28/50], Loss: 0.4573\n",
            "Epoch [28/50], Loss: 0.4747\n",
            "Epoch [28/50], Loss: 0.4747\n",
            "Epoch [28/50], Loss: 0.4573\n",
            "Epoch [28/50], Loss: 0.3536\n",
            "Epoch [28/50], Loss: 0.4054\n",
            "Epoch [28/50], Loss: 0.3881\n",
            "Epoch [28/50], Loss: 0.4401\n",
            "Epoch [28/50], Loss: 0.4922\n",
            "Epoch [29/50], Loss: 0.4401\n",
            "Epoch [29/50], Loss: 0.3707\n",
            "Epoch [29/50], Loss: 0.4574\n",
            "Epoch [29/50], Loss: 0.3708\n",
            "Epoch [29/50], Loss: 0.4227\n",
            "Epoch [29/50], Loss: 0.3707\n",
            "Epoch [29/50], Loss: 0.4575\n",
            "Epoch [29/50], Loss: 0.4401\n",
            "Epoch [29/50], Loss: 0.4227\n",
            "Epoch [29/50], Loss: 0.4227\n",
            "Epoch [29/50], Loss: 0.3879\n",
            "Epoch [29/50], Loss: 0.5274\n",
            "Epoch [30/50], Loss: 0.4227\n",
            "Epoch [30/50], Loss: 0.4748\n",
            "Epoch [30/50], Loss: 0.4227\n",
            "Epoch [30/50], Loss: 0.4572\n",
            "Epoch [30/50], Loss: 0.3540\n",
            "Epoch [30/50], Loss: 0.4056\n",
            "Epoch [30/50], Loss: 0.4399\n",
            "Epoch [30/50], Loss: 0.4056\n",
            "Epoch [30/50], Loss: 0.4227\n",
            "Epoch [30/50], Loss: 0.4227\n",
            "Epoch [30/50], Loss: 0.4742\n",
            "Epoch [30/50], Loss: 0.3885\n",
            "Epoch [31/50], Loss: 0.3371\n",
            "Epoch [31/50], Loss: 0.3712\n",
            "Epoch [31/50], Loss: 0.4572\n",
            "Epoch [31/50], Loss: 0.4400\n",
            "Epoch [31/50], Loss: 0.4748\n",
            "Epoch [31/50], Loss: 0.4574\n",
            "Epoch [31/50], Loss: 0.3360\n",
            "Epoch [31/50], Loss: 0.4401\n",
            "Epoch [31/50], Loss: 0.5445\n",
            "Epoch [31/50], Loss: 0.3186\n",
            "Epoch [31/50], Loss: 0.4053\n",
            "Epoch [31/50], Loss: 0.5096\n",
            "Epoch [32/50], Loss: 0.3359\n",
            "Epoch [32/50], Loss: 0.4227\n",
            "Epoch [32/50], Loss: 0.5271\n",
            "Epoch [32/50], Loss: 0.5095\n",
            "Epoch [32/50], Loss: 0.2155\n",
            "Epoch [32/50], Loss: 0.3881\n",
            "Epoch [32/50], Loss: 0.5269\n",
            "Epoch [32/50], Loss: 0.4921\n",
            "Epoch [32/50], Loss: 0.3535\n",
            "Epoch [32/50], Loss: 0.3535\n",
            "Epoch [32/50], Loss: 0.5094\n",
            "Epoch [32/50], Loss: 0.4574\n",
            "Epoch [33/50], Loss: 0.4227\n",
            "Epoch [33/50], Loss: 0.3882\n",
            "Epoch [33/50], Loss: 0.4572\n",
            "Epoch [33/50], Loss: 0.5779\n",
            "Epoch [33/50], Loss: 0.3542\n",
            "Epoch [33/50], Loss: 0.4227\n",
            "Epoch [33/50], Loss: 0.4569\n",
            "Epoch [33/50], Loss: 0.4398\n",
            "Epoch [33/50], Loss: 0.3720\n",
            "Epoch [33/50], Loss: 0.3382\n",
            "Epoch [33/50], Loss: 0.3719\n",
            "Epoch [33/50], Loss: 0.4909\n",
            "Epoch [34/50], Loss: 0.4739\n",
            "Epoch [34/50], Loss: 0.3034\n",
            "Epoch [34/50], Loss: 0.4912\n",
            "Epoch [34/50], Loss: 0.4399\n",
            "Epoch [34/50], Loss: 0.3884\n",
            "Epoch [34/50], Loss: 0.5087\n",
            "Epoch [34/50], Loss: 0.3884\n",
            "Epoch [34/50], Loss: 0.4743\n",
            "Epoch [34/50], Loss: 0.4399\n",
            "Epoch [34/50], Loss: 0.4570\n",
            "Epoch [34/50], Loss: 0.3543\n",
            "Epoch [34/50], Loss: 0.3714\n",
            "Epoch [35/50], Loss: 0.5085\n",
            "Epoch [35/50], Loss: 0.4399\n",
            "Epoch [35/50], Loss: 0.4570\n",
            "Epoch [35/50], Loss: 0.3715\n",
            "Epoch [35/50], Loss: 0.4740\n",
            "Epoch [35/50], Loss: 0.4740\n",
            "Epoch [35/50], Loss: 0.3206\n",
            "Epoch [35/50], Loss: 0.4569\n",
            "Epoch [35/50], Loss: 0.4739\n",
            "Epoch [35/50], Loss: 0.3035\n",
            "Epoch [35/50], Loss: 0.3886\n",
            "Epoch [35/50], Loss: 0.4227\n",
            "Epoch [36/50], Loss: 0.3539\n",
            "Epoch [36/50], Loss: 0.5092\n",
            "Epoch [36/50], Loss: 0.4574\n",
            "Epoch [36/50], Loss: 0.2666\n",
            "Epoch [36/50], Loss: 0.4227\n",
            "Epoch [36/50], Loss: 0.4403\n",
            "Epoch [36/50], Loss: 0.3524\n",
            "Epoch [36/50], Loss: 0.5112\n",
            "Epoch [36/50], Loss: 0.4228\n",
            "Epoch [36/50], Loss: 0.5470\n",
            "Epoch [36/50], Loss: 0.4405\n",
            "Epoch [36/50], Loss: 0.3698\n",
            "Epoch [37/50], Loss: 0.3699\n",
            "Epoch [37/50], Loss: 0.2817\n",
            "Epoch [37/50], Loss: 0.3874\n",
            "Epoch [37/50], Loss: 0.4228\n",
            "Epoch [37/50], Loss: 0.4229\n",
            "Epoch [37/50], Loss: 0.4587\n",
            "Epoch [37/50], Loss: 0.3871\n",
            "Epoch [37/50], Loss: 0.5486\n",
            "Epoch [37/50], Loss: 0.4946\n",
            "Epoch [37/50], Loss: 0.3872\n",
            "Epoch [37/50], Loss: 0.4228\n",
            "Epoch [37/50], Loss: 0.5114\n",
            "Epoch [38/50], Loss: 0.3875\n",
            "Epoch [38/50], Loss: 0.5806\n",
            "Epoch [38/50], Loss: 0.4401\n",
            "Epoch [38/50], Loss: 0.4400\n",
            "Epoch [38/50], Loss: 0.3543\n",
            "Epoch [38/50], Loss: 0.3377\n",
            "Epoch [38/50], Loss: 0.4058\n",
            "Epoch [38/50], Loss: 0.4228\n",
            "Epoch [38/50], Loss: 0.4058\n",
            "Epoch [38/50], Loss: 0.4737\n",
            "Epoch [38/50], Loss: 0.4058\n",
            "Epoch [38/50], Loss: 0.4398\n",
            "Epoch [39/50], Loss: 0.4906\n",
            "Epoch [39/50], Loss: 0.3890\n",
            "Epoch [39/50], Loss: 0.3890\n",
            "Epoch [39/50], Loss: 0.5244\n",
            "Epoch [39/50], Loss: 0.4735\n",
            "Epoch [39/50], Loss: 0.4229\n",
            "Epoch [39/50], Loss: 0.3725\n",
            "Epoch [39/50], Loss: 0.3390\n",
            "Epoch [39/50], Loss: 0.4061\n",
            "Epoch [39/50], Loss: 0.4397\n",
            "Epoch [39/50], Loss: 0.3720\n",
            "Epoch [39/50], Loss: 0.4738\n",
            "Epoch [40/50], Loss: 0.4228\n",
            "Epoch [40/50], Loss: 0.3886\n",
            "Epoch [40/50], Loss: 0.5428\n",
            "Epoch [40/50], Loss: 0.4056\n",
            "Epoch [40/50], Loss: 0.3884\n",
            "Epoch [40/50], Loss: 0.4743\n",
            "Epoch [40/50], Loss: 0.4227\n",
            "Epoch [40/50], Loss: 0.4399\n",
            "Epoch [40/50], Loss: 0.3026\n",
            "Epoch [40/50], Loss: 0.5088\n",
            "Epoch [40/50], Loss: 0.3882\n",
            "Epoch [40/50], Loss: 0.4055\n",
            "Epoch [41/50], Loss: 0.4746\n",
            "Epoch [41/50], Loss: 0.3881\n",
            "Epoch [41/50], Loss: 0.3707\n",
            "Epoch [41/50], Loss: 0.4227\n",
            "Epoch [41/50], Loss: 0.3879\n",
            "Epoch [41/50], Loss: 0.4052\n",
            "Epoch [41/50], Loss: 0.4578\n",
            "Epoch [41/50], Loss: 0.3876\n",
            "Epoch [41/50], Loss: 0.5284\n",
            "Epoch [41/50], Loss: 0.4227\n",
            "Epoch [41/50], Loss: 0.3525\n",
            "Epoch [41/50], Loss: 0.4931\n",
            "Epoch [42/50], Loss: 0.4403\n",
            "Epoch [42/50], Loss: 0.3526\n",
            "Epoch [42/50], Loss: 0.3701\n",
            "Epoch [42/50], Loss: 0.4052\n",
            "Epoch [42/50], Loss: 0.5283\n",
            "Epoch [42/50], Loss: 0.4227\n",
            "Epoch [42/50], Loss: 0.4227\n",
            "Epoch [42/50], Loss: 0.3527\n",
            "Epoch [42/50], Loss: 0.5454\n",
            "Epoch [42/50], Loss: 0.3878\n",
            "Epoch [42/50], Loss: 0.4576\n",
            "Epoch [42/50], Loss: 0.4053\n",
            "Epoch [43/50], Loss: 0.4748\n",
            "Epoch [43/50], Loss: 0.3881\n",
            "Epoch [43/50], Loss: 0.4227\n",
            "Epoch [43/50], Loss: 0.3710\n",
            "Epoch [43/50], Loss: 0.3366\n",
            "Epoch [43/50], Loss: 0.3536\n",
            "Epoch [43/50], Loss: 0.5268\n",
            "Epoch [43/50], Loss: 0.5096\n",
            "Epoch [43/50], Loss: 0.3880\n",
            "Epoch [43/50], Loss: 0.4400\n",
            "Epoch [43/50], Loss: 0.4054\n",
            "Epoch [43/50], Loss: 0.4746\n",
            "Epoch [44/50], Loss: 0.4054\n",
            "Epoch [44/50], Loss: 0.4227\n",
            "Epoch [44/50], Loss: 0.3020\n",
            "Epoch [44/50], Loss: 0.3881\n",
            "Epoch [44/50], Loss: 0.5269\n",
            "Epoch [44/50], Loss: 0.4227\n",
            "Epoch [44/50], Loss: 0.4574\n",
            "Epoch [44/50], Loss: 0.4054\n",
            "Epoch [44/50], Loss: 0.4054\n",
            "Epoch [44/50], Loss: 0.5094\n",
            "Epoch [44/50], Loss: 0.3881\n",
            "Epoch [44/50], Loss: 0.4573\n",
            "Epoch [45/50], Loss: 0.3710\n",
            "Epoch [45/50], Loss: 0.3882\n",
            "Epoch [45/50], Loss: 0.4400\n",
            "Epoch [45/50], Loss: 0.3882\n",
            "Epoch [45/50], Loss: 0.4746\n",
            "Epoch [45/50], Loss: 0.4227\n",
            "Epoch [45/50], Loss: 0.4746\n",
            "Epoch [45/50], Loss: 0.5436\n",
            "Epoch [45/50], Loss: 0.4227\n",
            "Epoch [45/50], Loss: 0.4227\n",
            "Epoch [45/50], Loss: 0.3376\n",
            "Epoch [45/50], Loss: 0.4058\n",
            "Epoch [46/50], Loss: 0.5079\n",
            "Epoch [46/50], Loss: 0.4737\n",
            "Epoch [46/50], Loss: 0.4736\n",
            "Epoch [46/50], Loss: 0.3723\n",
            "Epoch [46/50], Loss: 0.3388\n",
            "Epoch [46/50], Loss: 0.4397\n",
            "Epoch [46/50], Loss: 0.5240\n",
            "Epoch [46/50], Loss: 0.5070\n",
            "Epoch [46/50], Loss: 0.3727\n",
            "Epoch [46/50], Loss: 0.3561\n",
            "Epoch [46/50], Loss: 0.2723\n",
            "Epoch [46/50], Loss: 0.4566\n",
            "Epoch [47/50], Loss: 0.4736\n",
            "Epoch [47/50], Loss: 0.4398\n",
            "Epoch [47/50], Loss: 0.4058\n",
            "Epoch [47/50], Loss: 0.4398\n",
            "Epoch [47/50], Loss: 0.4056\n",
            "Epoch [47/50], Loss: 0.4399\n",
            "Epoch [47/50], Loss: 0.3540\n",
            "Epoch [47/50], Loss: 0.4400\n",
            "Epoch [47/50], Loss: 0.3881\n",
            "Epoch [47/50], Loss: 0.3879\n",
            "Epoch [47/50], Loss: 0.4053\n",
            "Epoch [47/50], Loss: 0.5104\n",
            "Epoch [48/50], Loss: 0.4227\n",
            "Epoch [48/50], Loss: 0.5634\n",
            "Epoch [48/50], Loss: 0.2825\n",
            "Epoch [48/50], Loss: 0.4052\n",
            "Epoch [48/50], Loss: 0.4754\n",
            "Epoch [48/50], Loss: 0.4227\n",
            "Epoch [48/50], Loss: 0.4929\n",
            "Epoch [48/50], Loss: 0.3177\n",
            "Epoch [48/50], Loss: 0.4052\n",
            "Epoch [48/50], Loss: 0.3877\n",
            "Epoch [48/50], Loss: 0.4227\n",
            "Epoch [48/50], Loss: 0.4931\n",
            "Epoch [49/50], Loss: 0.4579\n",
            "Epoch [49/50], Loss: 0.4578\n",
            "Epoch [49/50], Loss: 0.4402\n",
            "Epoch [49/50], Loss: 0.4227\n",
            "Epoch [49/50], Loss: 0.5269\n",
            "Epoch [49/50], Loss: 0.4400\n",
            "Epoch [49/50], Loss: 0.4056\n",
            "Epoch [49/50], Loss: 0.3202\n",
            "Epoch [49/50], Loss: 0.4228\n",
            "Epoch [49/50], Loss: 0.3374\n",
            "Epoch [49/50], Loss: 0.4912\n",
            "Epoch [49/50], Loss: 0.3714\n",
            "Epoch [50/50], Loss: 0.4913\n",
            "Epoch [50/50], Loss: 0.3542\n",
            "Epoch [50/50], Loss: 0.3540\n",
            "Epoch [50/50], Loss: 0.5262\n",
            "Epoch [50/50], Loss: 0.4227\n",
            "Epoch [50/50], Loss: 0.3709\n",
            "Epoch [50/50], Loss: 0.4227\n",
            "Epoch [50/50], Loss: 0.4400\n",
            "Epoch [50/50], Loss: 0.4401\n",
            "Epoch [50/50], Loss: 0.3533\n",
            "Epoch [50/50], Loss: 0.3879\n",
            "Epoch [50/50], Loss: 0.5275\n"
          ]
        }
      ],
      "source": [
        "# Utility function to train the model\n",
        "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
        "    # correct = 0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    # Repeat for given number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "        # Train with batches of data\n",
        "        for xb,yb in train_dl:\n",
        "          # try:\n",
        "            # 1. Generate predictions\n",
        "            pred = model(xb)\n",
        "            # print(pred.flatten() , yb)\n",
        "            # 2. Calculate loss\n",
        "            # asi = torch.argmax(pred,axis=1)\n",
        "            # print(asi.type(torch.float32), yb)\n",
        "            # break\n",
        "            loss = loss_fn(pred.flatten(), yb.type(torch.float32))\n",
        "\n",
        "\n",
        "            # y_pred_tag = torch.round(pred)\n",
        "            # correct_results_sum = (y_pred_tag == yb).sum().float()\n",
        "            # acc = correct_results_sum/yb.shape[0]\n",
        "            # acc = torch.round(acc * 100)\n",
        "\n",
        "            # 3. Compute gradients\n",
        "            loss.backward() # back propogation\n",
        "\n",
        "            # 4. Update parameters using gradients\n",
        "            opt.step() # it is update the weight for gradients\n",
        "\n",
        "            # 5. Reset the gradients to zero\n",
        "            opt.zero_grad() # for the zero the gradient bez new calculation of the neuron\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # Print the progress\n",
        "        # if (epoch+1) % 5 == 0:\n",
        "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "\n",
        "          # except:\n",
        "          #   return xb\n",
        "\n",
        "d = fit(50, model, criterion, optimizer, train_loader)\n",
        "d"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E5TBOchc2NMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTiA_CtdYjuw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nTORpRKaHlr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e5c64b5-275c-4208-a859-69181cc23202"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ham']"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "# sen = 'Aaooooright are you at work?'\n",
        "sen =df['text'][ind]\n",
        "a = [word_dict[i] for i in sen.split()]\n",
        "a = torch.tensor(a, dtype=torch.int64)\n",
        "testing = torch.nn.functional.pad(a, (0, pdd - len(a))) # 0: No padding is added to the left (or beginning) of the tensor\n",
        "out = model(testing.unsqueeze(0)) # adds a new dimension at the beginning, so it represents a batch with one sample of three elements.\n",
        "['spam' if out< 0.50 else 'ham']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ind = 127\n",
        "print(df.iloc[ind,:])\n"
      ],
      "metadata": {
        "id": "gETlPKiywxr1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdebd581-5c10-400b-fa22-0c19b705dee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class                       ham\n",
            "text     Are you there in room.\n",
            "Name: 127, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[:200,0].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "K0ksDksjY6-R",
        "outputId": "04b1e62d-6b4c-4e3e-b723-08894a4cc7fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "class\n",
              "ham     167\n",
              "spam     33\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>class</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ham</th>\n",
              "      <td>167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>spam</th>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# THEORY\n",
        "\"\"\"torch.nn.RNN is a class in PyTorch that implements a basic recurrent neural network (RNN) layer.\n",
        " RNNs are designed to process sequential data by maintaining a hidden state that captures information from previous time steps,\n",
        " making them suitable for tasks such as time series prediction, natural language processing (NLP), and more.\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None)\n",
        "num_embeddings: The number of unique items in the input space. For example, if you're embedding words in a vocabulary of 10,000 unique words, this value would be 10,000.\n",
        "\n",
        "embedding_dim: The dimensionality of the embedding vectors, which defines the size of the vector space in which the items will be represented. For instance, setting embedding_dim=50 would represent each word as a 50-dimensional vector.\n",
        "\n",
        "padding_idx: (Optional) Specifies an index to be treated as a padding token. When a tensor containing this index is passed, it will be ignored in gradient calculations (useful in NLP when handling variable-length sequences).\n",
        "\n",
        "\n",
        "#RNN\n",
        "torch.nn.RNN is a class in PyTorch that implements a basic recurrent neural network (RNN) layer.\n",
        "RNNs are designed to process sequential data by maintaining a hidden state that captures information from previous time steps,\n",
        "making them suitable for tasks such as time series prediction, natural language processing (NLP), and more.\n",
        "\n",
        "The class takes several parameters:\n",
        "input_size: The number of expected features in the input. For example, if you are working with word embeddings of size 50, input_size would be 50.\n",
        "hidden_size: The number of features in the hidden state. This determines the size of the hidden state vector that the RNN will produce.\n",
        "num_layers: The number of recurrent layers. A value of 1 means a single layer, while a value of 2 or more stacks additional RNN layers on top of each other.\n",
        "nonlinearity: The activation function to use. The default is 'tanh', but you can also use 'relu' if desired.\n",
        "bias: If True, adds a learnable bias to the output. Default is True.\n",
        "batch_first: If True, then the input and output tensors are provided as (batch, seq, feature). If False, they are provided as (seq, batch, feature).\n",
        "\n",
        "Purpose of a Linear Layer\n",
        "Functionality: The primary purpose of a linear layer is to perform a linear transformation on the input data.\n",
        "nn.Linear(32, 1, bias=False):\n",
        "\n",
        "This initializes a linear layer with specific parameters:\n",
        "32: The size of the input features. This means that the layer expects input tensors with 32 features. Each input sample should be a vector of size 32.\n",
        "1: The size of the output features. This means that the layer will produce an output with a single feature. In many cases, this might represent a scalar value (like a prediction).\n",
        "bias=False: This specifies that the layer should not include a bias term. In a typical linear layer, the output is calculated as:\n",
        "output = WX+B\n",
        "b is the bias vector. By setting bias=False, the model will not learn an additional bias term during training.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nSdLHEtjkSjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3M4PVkgEkv_R"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}